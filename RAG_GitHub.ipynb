{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1fnckJYk76EAKA9h275XOF81j1LKqVjEE",
      "authorship_tag": "ABX9TyOzVeBNRzSXvhDK/Kli2AhJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanjichka/RAG/blob/main/RAG_GitHub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-google-genai langchain-chroma langchain-community\n",
        "!pip install python-dotenv unstructured python-docx\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import hashlib\n",
        "from typing import List\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from docx import Document as DocxDocument\n",
        "from langchain_core.documents import Document\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "load_dotenv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GEYxnzruembz",
        "outputId": "2f92b57f-8a14-4fc1-b33b-592f32430fd5",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-4.2.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting langchain-chroma\n",
            "  Downloading langchain_chroma-1.1.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-genai<2.0.0,>=1.56.0 (from langchain-google-genai)\n",
            "  Downloading google_genai-1.59.0-py3-none-any.whl.metadata (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain-core<2.0.0,>=1.2.5 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.2.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (2.12.3)\n",
            "Collecting chromadb<2.0.0,>=1.3.5 (from langchain-chroma)\n",
            "  Downloading chromadb-1.4.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from langchain-chroma) (2.0.2)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
            "  Downloading langchain_classic-1.0.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.45)\n",
            "Collecting requests<3.0.0,>=2.32.5 (from langchain-community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Collecting build>=1.0.3 (from chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading build-1.4.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting pybase64>=1.4.1 (from chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.40.0)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (4.15.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.37.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.37.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.22.2)\n",
            "Collecting pypika>=0.48.9 (from chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading pypika-0.50.0-py2.py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.76.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.21.1)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading kubernetes-35.0.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (3.11.5)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<2.0.0,>=1.3.5->langchain-chroma) (4.26.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (4.12.1)\n",
            "Collecting google-auth<3.0.0,>=2.47.0 (from google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai)\n",
            "  Downloading google_auth-2.47.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (15.0.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.3.1)\n",
            "Collecting langchain-text-splitters<2.0.0,>=1.1.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n",
            "  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (25.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (0.13.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2026.1.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.5->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.30.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (2.9.0.post0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.9.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (2.0.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb<2.0.0,>=1.3.5->langchain-chroma) (25.12.19)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb<2.0.0,>=1.3.5->langchain-chroma) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.14.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (8.7.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.72.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.39.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.39.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading opentelemetry_proto-1.39.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading opentelemetry_api-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.60b1 (from opentelemetry-sdk>=1.2.0->chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.36.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.5.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.7.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb<2.0.0,>=1.3.5->langchain-chroma) (3.20.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb<2.0.0,>=1.3.5->langchain-chroma) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.6.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb<2.0.0,>=1.3.5->langchain-chroma)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb<2.0.0,>=1.3.5->langchain-chroma) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb<2.0.0,>=1.3.5->langchain-chroma) (1.3.0)\n",
            "Downloading langchain_google_genai-4.2.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_chroma-1.1.0-py3-none-any.whl (12 kB)\n",
            "Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-1.4.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_genai-1.59.0-py3-none-any.whl (719 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.1/719.1 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_classic-1.0.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.4.0-py3-none-any.whl (24 kB)\n",
            "Downloading google_auth-2.47.0-py3-none-any.whl (234 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m234.9/234.9 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-35.0.0-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
            "Downloading marshmallow-3.26.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.39.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.39.1-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.39.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl (219 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypika-0.50.0-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypika, filetype, durationpy, requests, pyproject_hooks, pybase64, opentelemetry-proto, mypy-extensions, marshmallow, humanfriendly, bcrypt, backoff, typing-inspect, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, google-auth, coloredlogs, build, opentelemetry-semantic-conventions, onnxruntime, kubernetes, dataclasses-json, opentelemetry-sdk, google-genai, opentelemetry-exporter-otlp-proto-grpc, langchain-text-splitters, langchain-google-genai, langchain-classic, chromadb, langchain-community, langchain-chroma\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: opentelemetry-proto\n",
            "    Found existing installation: opentelemetry-proto 1.37.0\n",
            "    Uninstalling opentelemetry-proto-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-proto-1.37.0\n",
            "  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n",
            "    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n",
            "    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.37.0\n",
            "    Uninstalling opentelemetry-api-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.37.0\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.43.0\n",
            "    Uninstalling google-auth-2.43.0:\n",
            "      Successfully uninstalled google-auth-2.43.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.37.0\n",
            "    Uninstalling opentelemetry-sdk-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.37.0\n",
            "  Attempting uninstall: google-genai\n",
            "    Found existing installation: google-genai 1.55.0\n",
            "    Uninstalling google-genai-1.55.0:\n",
            "      Successfully uninstalled google-genai-1.55.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires google-auth==2.43.0, but you have google-auth 2.47.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-5.0.0 build-1.4.0 chromadb-1.4.1 coloredlogs-15.0.1 dataclasses-json-0.6.7 durationpy-0.10 filetype-1.2.0 google-auth-2.47.0 google-genai-1.59.0 humanfriendly-10.0 kubernetes-35.0.0 langchain-chroma-1.1.0 langchain-classic-1.0.1 langchain-community-0.4.1 langchain-google-genai-4.2.0 langchain-text-splitters-1.1.0 marshmallow-3.26.2 mypy-extensions-1.1.0 onnxruntime-1.23.2 opentelemetry-api-1.39.1 opentelemetry-exporter-otlp-proto-common-1.39.1 opentelemetry-exporter-otlp-proto-grpc-1.39.1 opentelemetry-proto-1.39.1 opentelemetry-sdk-1.39.1 opentelemetry-semantic-conventions-0.60b1 posthog-5.4.0 pybase64-1.4.3 pypika-0.50.0 pyproject_hooks-1.2.0 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "725569e1ebf8489e8fbbd9a7e7738040"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Collecting unstructured\n",
            "  Downloading unstructured-0.18.27-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from unstructured) (3.4.4)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.12/dist-packages (from unstructured) (1.2.0)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from unstructured) (6.0.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from unstructured) (3.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from unstructured) (2.32.5)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from unstructured) (4.13.5)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.12/dist-packages (from unstructured) (0.6.7)\n",
            "Collecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2025.11.16-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unstructured) (2.0.2)\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.12/dist-packages (from unstructured) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from unstructured) (4.15.0)\n",
            "Collecting unstructured-client (from unstructured)\n",
            "  Downloading unstructured_client-0.42.8-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from unstructured) (2.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unstructured) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unstructured) (5.9.5)\n",
            "Collecting python-oxmsg (from unstructured)\n",
            "  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.12/dist-packages (from unstructured) (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->unstructured) (2.8.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->unstructured) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->unstructured) (0.9.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.12/dist-packages (from html5lib->unstructured) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from html5lib->unstructured) (0.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->unstructured) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->unstructured) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->unstructured) (2025.11.3)\n",
            "Collecting olefile (from python-oxmsg->unstructured)\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->unstructured) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->unstructured) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->unstructured) (2026.1.4)\n",
            "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured) (24.1.0)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured) (43.0.3)\n",
            "Requirement already satisfied: httpcore>=1.0.9 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured) (1.0.9)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=2.11.2 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured) (2.12.3)\n",
            "Collecting pypdf>=6.2.0 (from unstructured-client->unstructured)\n",
            "  Downloading pypdf-6.6.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured) (1.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=3.1->unstructured-client->unstructured) (2.0.0)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore>=1.0.9->unstructured-client->unstructured) (0.16.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (4.12.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.12/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (0.4.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.23)\n",
            "Downloading unstructured-0.18.27-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_iso639-2025.11.16-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.8/167.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Downloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\n",
            "Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_client-0.42.8-py3-none-any.whl (219 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.6.0-py3-none-any.whl (328 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.0/329.0 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=5f3f46971d70f27807316dc089e5c57f1a674462661abaf7b5a71c852cb4eacf\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
            "Successfully built langdetect\n",
            "Installing collected packages: rapidfuzz, python-magic, python-iso639, python-docx, pypdf, olefile, langdetect, emoji, python-oxmsg, unstructured-client, unstructured\n",
            "Successfully installed emoji-2.15.0 langdetect-1.0.9 olefile-0.47 pypdf-6.6.0 python-docx-1.2.0 python-iso639-2025.11.16 python-magic-0.4.27 python-oxmsg-0.0.2 rapidfuzz-3.14.3 unstructured-0.18.27 unstructured-client-0.42.8\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CHROMA_PATH = \"chroma\"\n",
        "DATA_PATH = \"data\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "W3PFN4mMoPCq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding_function():\n",
        "    return GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")"
      ],
      "metadata": {
        "id": "sNPs4SIIoQV6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_documents():\n",
        "    documents = []\n",
        "\n",
        "    for filename in os.listdir(DATA_PATH):\n",
        "        if filename.endswith(\".docx\"):\n",
        "            full_path = os.path.join(DATA_PATH, filename)\n",
        "            docx = DocxDocument(full_path)\n",
        "\n",
        "            current_title = \"General\"  # Fallback title\n",
        "            current_content = \"\"\n",
        "\n",
        "            for para in docx.paragraphs:\n",
        "                # Detect Word Heading styles\n",
        "                if para.style.name.startswith(\"Heading\"):\n",
        "                    if current_content.strip():\n",
        "                        documents.append(Document(\n",
        "                            page_content=current_content.strip(),\n",
        "                            metadata={\n",
        "                                \"title\": current_title,\n",
        "                                \"source\": filename\n",
        "                            }\n",
        "                        ))\n",
        "                    current_title = para.text.strip()\n",
        "                    current_content = \"\"\n",
        "                else:\n",
        "                    # Collect text under the current heading\n",
        "                    if para.text.strip():\n",
        "                        current_content += para.text + \"\\n\"\n",
        "\n",
        "            # Append the final section of the document\n",
        "            if current_content.strip():\n",
        "                documents.append(Document(\n",
        "                    page_content=current_content.strip(),\n",
        "                    metadata={\n",
        "                        \"title\": current_title,\n",
        "                        \"source\": filename\n",
        "                    }\n",
        "                ))\n",
        "\n",
        "    return documents"
      ],
      "metadata": {
        "id": "cPi7FziJoS-6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hash_content(text: str) -> str:\n",
        "    return hashlib.sha256(text.encode()).hexdigest()"
      ],
      "metadata": {
        "id": "yjYfPxWX7a3q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add unique ID to each text chunk.\n",
        "chunks = load_documents()\n",
        "\n",
        "def calculate_chunk_ids(chunks: List[Document]):\n",
        "    last_page_id = None\n",
        "    current_chunk_index = 0\n",
        "\n",
        "    for idx, chunk in enumerate(chunks):\n",
        "        source = chunk.metadata.get(\"source\", \"unknown\")\n",
        "        chunk_id = f\"{source}:{idx}\"\n",
        "        chunk.metadata[\"id\"] = chunk_id\n",
        "        chunk.metadata[\"content_hash\"] = hash_content(chunk.page_content)\n",
        "    return chunks\n",
        "\n",
        "chunks_with_ids = calculate_chunk_ids(chunks)"
      ],
      "metadata": {
        "id": "utxLuajBoUS5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chunks[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXVmGDPzsU-x",
        "outputId": "887f70ee-49b1-4952-9d52-ef0a7a5d162e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='Understanding how BigQuery handles data is crucial for efficient analysis.\n",
            "Separation of Compute and Storage: This allows you to store vast amounts of data without over-provisioning compute power, keeping costs manageable.\n",
            "Dremel Query Engine: BigQuery uses the Dremel engine to execute queries in parallel across thousands of workers.\n",
            "Columnar Storage: Data is stored by column rather than by row, significantly reducing the amount of data read during a query and improving performance for large-scale analytical tasks.' metadata={'title': '2. Core Architecture and Data Processing', 'source': 'BigQuery for Large-Scale Healthcare Analytics.docx', 'id': 'BigQuery for Large-Scale Healthcare Analytics.docx:1', 'content_hash': '5537cc60e7f7c319b4b2a71c1ed045fa94159e1469e789581b6a20a0ff01eecb'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chunks_with_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdLeiMjMEefd",
        "outputId": "d26ed900-04ff-4cad-8b93-1eacdb6fa000"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'title': '1. Introduction to Google BigQuery', 'source': 'BigQuery for Large-Scale Healthcare Analytics.docx', 'id': 'BigQuery for Large-Scale Healthcare Analytics.docx:0', 'content_hash': '7fe2f5dcf137cac098c455c38979b4b0bd8e6ccde3b012889007d9eea92cf291'}, page_content='Google BigQuery is a serverless, highly scalable data warehouse designed for high-performance analytics on massive datasets. Unlike traditional relational databases (OLTP), BigQuery is optimized for Online Analytical Processing (OLAP), allowing you to run complex SQL queries on petabytes of data in seconds. Its core architecture separates compute and storage, which provides the flexibility to scale resources independently as your data needs grow.'), Document(metadata={'title': '2. Core Architecture and Data Processing', 'source': 'BigQuery for Large-Scale Healthcare Analytics.docx', 'id': 'BigQuery for Large-Scale Healthcare Analytics.docx:1', 'content_hash': '5537cc60e7f7c319b4b2a71c1ed045fa94159e1469e789581b6a20a0ff01eecb'}, page_content='Understanding how BigQuery handles data is crucial for efficient analysis.\\nSeparation of Compute and Storage: This allows you to store vast amounts of data without over-provisioning compute power, keeping costs manageable.\\nDremel Query Engine: BigQuery uses the Dremel engine to execute queries in parallel across thousands of workers.\\nColumnar Storage: Data is stored by column rather than by row, significantly reducing the amount of data read during a query and improving performance for large-scale analytical tasks.'), Document(metadata={'title': '3. Data Ingestion Strategies (ETL vs. ELT)', 'source': 'BigQuery for Large-Scale Healthcare Analytics.docx', 'id': 'BigQuery for Large-Scale Healthcare Analytics.docx:2', 'content_hash': '2ed480002f9841a5e623851eb13c7af26ca64bcfc262f5af3af22b094fd3e91b'}, page_content='For your research projects, such as analyzing the PTB-XL ECG dataset, you can choose from several workflows:\\nEL (Extract and Load): Best for periodic loads of historical log files.\\nETL (Extract, Transform, Load): Useful when data needs quality control or enrichment before loading, often using tools like Apache Beam or Cloud Dataflow.\\nELT (Extract, Load, Transform): Ideal for exploratory work where transformations are handled on-the-fly via BigQuery views.'), Document(metadata={'title': '4. Query Essentials and Advanced SQL', 'source': 'BigQuery for Large-Scale Healthcare Analytics.docx', 'id': 'BigQuery for Large-Scale Healthcare Analytics.docx:3', 'content_hash': '5ddba8b4530637e70cdef53a46eed23516223ded1155c0059c03bb0e6c6d3c6d'}, page_content='BigQuery supports standard SQL:2011, which is familiar to most data professionals. Key operations include:\\nAggregates: Using GROUP BY, AVG, and COUNT to summarize clinical parameters like patient age or heart rate trends.\\nJoins: Efficiently combining \"fact\" tables (e.g., patient measurements) with \"dimension\" tables (e.g., medical diagnoses).\\nArrays and Structs: Handling hierarchical data, such as diagnostic codes (SCP codes), without needing to flatten the dataset into a single row.'), Document(metadata={'title': '5. Machine Learning Integration (BigQuery ML)', 'source': 'BigQuery for Large-Scale Healthcare Analytics.docx', 'id': 'BigQuery for Large-Scale Healthcare Analytics.docx:4', 'content_hash': '648d47f5716e4e2c64761c1e8d4a847f010dc10ef53bf92cce48dcd220bb7ca7'}, page_content=\"One of the most powerful features for your Master's thesis is BigQuery ML. You can train and evaluate machine learning models—like those for predicting heart failure or Atrial Fibrillation—directly within BigQuery using SQL. This eliminates the need to export data to external tools, enhancing security and maintaining data locality.\"), Document(metadata={'title': '6. Visualizing Insights with Looker Studio', 'source': 'BigQuery for Large-Scale Healthcare Analytics.docx', 'id': 'BigQuery for Large-Scale Healthcare Analytics.docx:5', 'content_hash': '885a9a7fd591aa963760672dd7d9c53ca8e775e273986c204f8a5af3f601f97e'}, page_content='Once your analysis is complete, you can connect BigQuery to Looker Studio to create interactive dashboards. For example, in your project on PTB-XL data, you used this integration to visualize demographic characteristics and the frequency of diagnoses across different patient age groups')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_to_chroma(chunks: List[Document]):\n",
        "\n",
        "    db = Chroma(\n",
        "        persist_directory=CHROMA_PATH,\n",
        "        embedding_function=get_embedding_function()\n",
        "    )\n",
        "\n",
        "    chunks_with_ids = calculate_chunk_ids(chunks)\n",
        "\n",
        "    # Get existing data\n",
        "    existing_items = db.get(include=[\"metadatas\"])\n",
        "    existing_ids = set(existing_items[\"ids\"])\n",
        "    existing_metadatas = existing_items[\"metadatas\"]\n",
        "\n",
        "    # Map existing ID to its hash for comparison\n",
        "    existing_hashes = {\n",
        "        id_: metadata.get(\"content_hash\")\n",
        "        for id_, metadata in zip(existing_items[\"ids\"], existing_metadatas)\n",
        "    }\n",
        "\n",
        "    print(f\"Number of existing documents in DB: {len(existing_hashes)}\")\n",
        "\n",
        "    # Only add chunks if the ID is new OR the content (hash) has changed\n",
        "    new_chunks = []\n",
        "    for chunk in chunks_with_ids:\n",
        "        chunk_id = chunk.metadata[\"id\"]\n",
        "        chunk_hash = chunk.metadata[\"content_hash\"]\n",
        "\n",
        "        if chunk_id not in existing_hashes or existing_hashes[chunk_id] != chunk_hash:\n",
        "            new_chunks.append(chunk)\n",
        "\n",
        "    if new_chunks:\n",
        "        print(f\"👉 Adding/Updating {len(new_chunks)} chunks to Gemini index...\")\n",
        "        new_chunk_ids = [chunk.metadata[\"id\"] for chunk in new_chunks]\n",
        "        db.add_documents(new_chunks, ids=new_chunk_ids)\n",
        "        # Note: In recent LangChain/Chroma versions, persist() is called automatically.\n",
        "    else:\n",
        "        print(\"✅ No changes detected. Database is up to date.\")\n",
        "\n",
        "\n",
        "add_to_chroma(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78fCHtMeoVdh",
        "outputId": "75a40935-336a-413e-f10b-2063880dd639"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of existing documents in DB: 0\n",
            "👉 Adding/Updating 6 chunks to Gemini index...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wipes the vector DB stored in Chroma.\n",
        "\n",
        "# def clear_database():\n",
        "#     if os.path.exists(CHROMA_PATH):\n",
        "#         shutil.rmtree(CHROMA_PATH)\n",
        "#         print(\"Database cleared.\")\n",
        "#     else:\n",
        "#         print(\"Database path not found.\")\n",
        "\n",
        "# Run this if you want to reset:\n",
        "# clear_database()"
      ],
      "metadata": {
        "id": "gHzsqj9qoWi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "Answer the question based only on the following context:\n",
        "\n",
        "{context}\n",
        "\n",
        "---\n",
        "\n",
        "Answer the question in simple terms, be straightforward, and concise. Your answers must be based on the above context: {question}\n",
        "\"\"\"\n",
        "\n",
        "def query_rag(query_text: str):\n",
        "    # 1. Prepare the DB with Gemini Embeddings\n",
        "    embedding_function = get_embedding_function()\n",
        "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
        "\n",
        "    # 2. Search the DB\n",
        "    # Gemini 2.5 Flash/Pro are great at handling more context.\n",
        "    # You could even try k=5 or k=10 if your chunks are small.\n",
        "    results = db.similarity_search_with_score(query_text, k=3)\n",
        "\n",
        "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
        "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
        "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
        "\n",
        "    # 3. Call the Gemini Model\n",
        "    model = ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-2.5-flash-lite\",\n",
        "        temperature=0 # Low temperature is best for factual RAG\n",
        "    )\n",
        "\n",
        "    response = model.invoke(prompt)\n",
        "    response_text = response.content\n",
        "\n",
        "    # Optional: Logic to normalize the response\n",
        "    response_normalized = response_text.strip().lower()\n",
        "\n",
        "    # 4. Extract Sources\n",
        "    sources = [doc.metadata.get(\"id\", None) for doc, _score in results]\n",
        "\n",
        "    print(f\"Response: {response_text}\")\n",
        "    print(f\"Sources: {sources}\")\n",
        "\n",
        "    return response_normalized"
      ],
      "metadata": {
        "id": "WzkwRHgto-b2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_text = input(\"Enter your question: \")\n",
        "query_rag(query_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "Rtuej5dsrtfm",
        "outputId": "a2af3571-4cea-4bdb-cee2-fa93ee5c2cf7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question: Which standard SQL is supported in BigQuery?\n",
            "Response: BigQuery supports standard SQL:2011.\n",
            "Sources: ['BigQuery for Large-Scale Healthcare Analytics.docx:3', 'BigQuery for Large-Scale Healthcare Analytics.docx:0', 'BigQuery for Large-Scale Healthcare Analytics.docx:1']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bigquery supports standard sql:2011.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TESTING**"
      ],
      "metadata": {
        "id": "v0hwOzkt4K2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt used to evaluate model responses\n",
        "EVAL_PROMPT = \"\"\"\n",
        "Expected Response: {expected_response}\n",
        "Actual Response: {actual_response}\n",
        "---\n",
        "(Answer with 'true' or 'false') Is the actual response semantically equivalent to the expected response, even if phrased differently?\n",
        "\"\"\"\n",
        "\n",
        "def query_and_validate(question: str, expected_response: str):\n",
        "    # Get RAG response\n",
        "    response_message = query_rag(question)\n",
        "\n",
        "    if hasattr(response_message, \"content\"):\n",
        "        response_text = response_message.content\n",
        "    elif isinstance(response_message, dict):\n",
        "        response_text = response_message.get(\"content\", str(response_message))\n",
        "    else:\n",
        "        response_text = str(response_message)\n",
        "\n",
        "    actual_normalized = response_text.strip() if response_text else \"No response received\"\n",
        "\n",
        "    # Format the evaluation prompt\n",
        "    prompt = EVAL_PROMPT.format(\n",
        "        expected_response=expected_response, actual_response=actual_normalized\n",
        "    )\n",
        "\n",
        "    # Evaluate the response using ChatGoogleGenerativeAI\n",
        "    model = ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-2.5-flash-lite\",\n",
        "        temperature=0 # Low temperature is best for factual RAG\n",
        "    )\n",
        "    evaluation_results_str = model.invoke(prompt)\n",
        "    evaluation_results_str = evaluation_results_str.content  # Extract content\n",
        "    evaluation_results_str_cleaned = evaluation_results_str.strip().lower()\n",
        "\n",
        "    print(prompt)\n",
        "\n",
        "    # Output and validation\n",
        "    if \"true\" in evaluation_results_str_cleaned:\n",
        "        print(\"\\033[92m\" + f\"Response: {evaluation_results_str_cleaned}\" + \"\\033[0m\")\n",
        "        return True\n",
        "    elif \"false\" in evaluation_results_str_cleaned:\n",
        "        print(\"\\033[91m\" + f\"Response: {evaluation_results_str_cleaned}\" + \"\\033[0m\")\n",
        "        return False\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f\"Invalid evaluation result. Cannot determine if 'true' or 'false'.\"\n",
        "        )"
      ],
      "metadata": {
        "id": "DuoAX03KsVXM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_positive():\n",
        "    assert query_and_validate(\n",
        "        question=\"Which standard SQL is supported in BigQuery?\",\n",
        "        expected_response=\"SQL version 2011\",\n",
        "    )\n",
        "test_positive()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cax3pFpW4zxD",
        "outputId": "f3b7c729-9d1e-431b-a8eb-ed11534fff93"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: BigQuery supports standard SQL:2011.\n",
            "Sources: ['BigQuery for Large-Scale Healthcare Analytics.docx:3', 'BigQuery for Large-Scale Healthcare Analytics.docx:0', 'BigQuery for Large-Scale Healthcare Analytics.docx:1']\n",
            "\n",
            "Expected Response: SQL version 2011\n",
            "Actual Response: bigquery supports standard sql:2011.\n",
            "---\n",
            "(Answer with 'true' or 'false') Is the actual response semantically equivalent to the expected response, even if phrased differently?\n",
            "\n",
            "\u001b[92mResponse: true\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_negative_false():\n",
        "    assert query_and_validate(\n",
        "        question=\"Which standard SQL is supported in BigQuery?\",\n",
        "        expected_response=\"SQL 2010\",\n",
        "    )\n",
        "test_negative_false()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "4sDqxxZPBl5Y",
        "outputId": "6042db67-02ee-437d-fefa-9a7581926742"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: BigQuery supports standard SQL:2011.\n",
            "Sources: ['BigQuery for Large-Scale Healthcare Analytics.docx:3', 'BigQuery for Large-Scale Healthcare Analytics.docx:0', 'BigQuery for Large-Scale Healthcare Analytics.docx:1']\n",
            "\n",
            "Expected Response: SQL 2010\n",
            "Actual Response: bigquery supports standard sql:2011.\n",
            "---\n",
            "(Answer with 'true' or 'false') Is the actual response semantically equivalent to the expected response, even if phrased differently?\n",
            "\n",
            "\u001b[91mResponse: false\u001b[0m\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-523387017.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mexpected_response\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SQL 2010\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     )\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtest_negative_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-523387017.py\u001b[0m in \u001b[0;36mtest_negative_false\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_negative_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     assert query_and_validate(\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Which standard SQL is supported in BigQuery?\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mexpected_response\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SQL 2010\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     )\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_negative_true():\n",
        "    assert not query_and_validate(\n",
        "        question=\"Which standard SQL is supported in BigQuery?\",\n",
        "        expected_response=\"SQL 2010\",\n",
        "    )\n",
        "test_negative_true()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2FpfZUVPSxm",
        "outputId": "129aa15e-57e8-4b14-c6ef-06b69790c5c8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: BigQuery supports standard SQL:2011.\n",
            "Sources: ['BigQuery for Large-Scale Healthcare Analytics.docx:3', 'BigQuery for Large-Scale Healthcare Analytics.docx:0', 'BigQuery for Large-Scale Healthcare Analytics.docx:1']\n",
            "\n",
            "Expected Response: SQL 2010\n",
            "Actual Response: bigquery supports standard sql:2011.\n",
            "---\n",
            "(Answer with 'true' or 'false') Is the actual response semantically equivalent to the expected response, even if phrased differently?\n",
            "\n",
            "\u001b[91mResponse: false\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}